package main

// ======================================
// Тренажёр: Конкурентность в Go (v1)
// ======================================
// Во всех заданиях:
// • Всегда обрабатывай ошибки и корректно останавливай таймеры/тикеры.
// • Закрывает канал тот, кто отправляет. Чтение из закрытого канала даёт (zero, false).
// • Не опирайся на Sleep как на «синхронизацию» — используй каналы/WaitGroup/Context.

// --- Блок A. Горутины и планировщик ---

// 1. Запусти 3 горутины, каждая печатает свой номер. Дождайся завершения (WaitGroup).
// 2. Покажи гонку данных: общий счётчик без синхронизации в 1000 горутин. Запусти под -race и зафиксируй предупреждения.
// 3. Почини гонку из п.2 через mutex (см. блок I) — сравни результат и отсутствие гонок под -race.
// 4. Запусти горутину, которая возвращает результат через канал (future-паттерн): calc -> out chan int.
// 5. Передай в горутину копию переменной цикла (избеги захвата i по ссылке). Сравни с багом при захвате.

// --- Блок B. Каналы: базовое API ---

// 6. Сравни поведение буферизованного (cap=1) и небуферизованного канала при одиночной отправке без читателя (deadlock vs нет).
// 7. Отправь 3 значения в буферизованный канал cap=2 и объясни блокировку на третьем send до чтения.
// 8. Закрой канал на стороне отправителя и прочитай в цикле for v := range ch { ... } — зафиксируй завершение цикла.
// 9. Попробуй отправить в закрытый канал — поймай панику; сделай защиту (ok := recover в Safe).
// 10. Прочитай из nil-канала в select — покажи, что ветка никогда не срабатывает (deadlock).

// --- Блок C. select, timeouts и мультиплексирование ---

// 11. Реализуй ожидание первого из двух каналов (select { case v := <-a: ... case v := <-b: ... }).
// 12. Добавь таймаут ожидания через select + time.After; по таймауту верни ошибку.
// 13. Реализуй non-blocking try-send/try-recv через select с default.
// 14. Мультиплексируй ввод из N каналов в один общий (fan-in) через горутины и select.
// 15. Реализуй «heartbeat»: периодически отправляй ping в канал, пока контекст не отменён.

// --- Блок D. Таймеры и тикеры ---

// 16. Используй time.NewTimer для отложенного события; корректно останови Timer.Stop() и дренируй канал.
// 17. Перезапусти timer через Reset() — продемонстрируй корректный паттерн с drain перед Reset.
// 18. Используй time.Ticker для периодических задач; корректно останови через ticker.Stop().
// 19. Реализуй debouncing: события приходят часто, обрабатывай только, если 300ms тишины (timer-based).
// 20. Сравни time.After (утечка при длительном ожидании в цикле) vs один общий Timer с Reset.

// --- Блок E. WaitGroup и порядок Add/Done ---

// 21. Неправильный порядок: wg.Add внутри горутины — покажи потенциальную гонку/панику. Исправь: Add до запуска.
// 22. Инкапсулируй шаблон: Run(wg *sync.WaitGroup, fn func()) — внутри defer wg.Done().
// 23. Покажи deadlock при Wait, если кто-то забыл Done; добавь контекст/таймер для диагностики.
// 24. Сядронь запуск N задач с WaitGroup и сбором результатов в закрываемый канал.

// --- Блок F. Context: отмена и дедлайны ---

// 25. Запусти работу с context.WithTimeout. По истечении срока верни context.DeadlineExceeded.
// 26. Реализуй ручную отмену через context.WithCancel — отменяй по событию с канала.
// 27. Пробрось ctx через 2 уровня функций — в глубине уважай ctx.Done() в select.
// 28. Сравни поведение: time.After(timeout) vs ctx с таймаутом, где отмена может быть внешней.
// 29. Реализуй graceful-shutdown: сигнал os.Interrupt -> cancel -> жди завершения горутин с WaitGroup.

// --- Блок G. Паттерны каналов: fan-out/in, пайплайны, worker pool ---

// 30. Построй пайплайн: gen(ints) -> square -> sum. Каждый этап — своя горутина/канал.
// 31. Fan-out: запусти K воркеров, читающих из одного входного канала, результаты в общий выход (fan-in).
// 32. Worker pool с ограничением параллелизма K, задач M: по окончании закрой выходной канал.
// 33. Добавь контекст в pool: первая ошибка отменяет остальные задачи; аккуратно закрывай каналы.
// 34. Реализуй backpressure: если потребитель не успевает, производитель блокируется — покажи на cap=0 и cap>0.
// 35. Очередь с приоритетами: два канала (high/low) и select с предпочтением high (default + requeue).

// --- Блок H. Rate limiting и семафоры ---

// 36. Токен-бакет на тикере: выпуск 5 токенов/сек, обработай 20 задач с ограничением по скорости.
// 37. Ограничь одновременный доступ к ресурсу через семафор (chan struct{} с cap=3).
// 38. Сделай per-key ограничение: map[key]chan struct{}, где по каждому ключу параллелизм 1.
// 39. Реализуй лимит «X запросов в окно T» с time.Now() и кольцевым буфером таймстампов.
// 40. Сравни SetLimit у errgroup (если знаешь) и семафор — где удобнее, где гибче (комментариями).

// --- Блок I. Mutex, RWMutex, Cond ---

// 41. Инкрементируй общий счётчик в 1000 горутин с sync.Mutex; покажи отсутствие гонок.
// 42. Реализуй thread-safe map[string]int с RWMutex: Get/Set/Len.
// 43. Покажи апгрейд-проблему: нельзя держать RLock и потом Lock — продемонстрируй потенциальный дедлок (закомментируй и объясни).
// 44. Используй sync.Cond для ожидания, пока очередь непуста: Prod/Cons с сигналами Signal/Broadcast.
// 45. Сравни производительность RWMutex vs Mutex при преобладании чтений (микробенчмарк).

// --- Блок J. sync/atomic и atomic.Value ---

// 46. Гонка счётчика: почини через atomic.AddInt64(&n, 1). Сверь с Mutex по скорости (микробенчмарк).
// 47. Реализуй флаг остановки через atomic.Bool (Go 1.19+): Store(true)/Load() в цикле.
// 48. Используй atomic.Value для безопасной публикации неизменяемой конфигурации; читатели без блокировок.
// 49. Покажи ABA-проблему (концептуально) и где atomic.CompareAndSwap пригодится (краткий комментарий + мини-пример).
// 50. Реализуй счетчик с периодическим сбросом в отдельной горутине: инкременты atomic, чтение/сброс по тикеру.

// --- Блок K. sync.Map — когда и как ---

// 51. Создай sync.Map, положи 3 ключа, получи Load, LoadOrStore, Delete. Пройдись Range и собери отсортированные ключи в []string.
// 52. Сравни поведение обычной map с RWMutex и sync.Map при шаблоне "много чтений, редкие записи". Снимай время.
// 53. Покажи, что тип ключей/значений — any; добавь типобезопасные обёртки GetString/SetString вокруг sync.Map.
// 54. Реализуй «одноразовую инициализацию» на ключ: если LoadOrStore вернул existing, не пересоздавай ресурс.
// 55. Обсуди (комментарием) анти-кейс: маленькие фиксированные наборы ключей — лучше обычная map с RWMutex.

// --- Блок L. sync.Pool — повторное использование объектов ---

// 56. Создай sync.Pool для *bytes.Buffer; в New выделяй новый. В горутине возьми, запиши данные, Reset, Put обратно.
// 57. Покажи, что GC может очищать Pool; нельзя полагаться на наличие объекта — всегда готовься к New.
// 58. Сравни выделения/время с/без Pool в микробенчмарке при создании множества временных буферов.
// 59. Напиши безопасную обёртку GetBuf()/PutBuf(*bytes.Buffer), запрещая использовать буфер после Put (комментарий и тест).
// 60. Используй Pool в worker pool для переиспользования временных структур задачи.

// --- Блок M. Отладка и диагностика ---

// 61. Воспроизведи deadlock (взаимная блокировка двух goroutine на двух мьютексах) — запусти и зафиксируй сообщение runtime о deadlock.
// 62. Используй runtime.NumGoroutine() до/после — убедись, что нет утечек горутин после завершения пайплайна.
// 63. Покажи утечку: горутина ждёт на recv из канала, который никто не закрывает; почини через контекст/закрытие.
// 64. Сними pprof goroutine profile (net/http/pprof) в простом сервисе, посмотри заголовки стектрейсов (кратко).
// 65. Добавь лог-теги/trace id в сообщения горутин, чтобы понимать, кто что делает (паттерн структурированного лога).

// --- Блок N. Практикум «из жизни» ---

// 66. Ограниченный downloader: K параллельных скачиваний URL (заглушки), таймаут per-URL, общий контекст отмены.
// 67. Параллельный парсер файлов: producer читает пути, воркеры парсят, агрегатор пишет результаты в map (с mutex).
// 68. Пайплайн ETL: read -> transform -> write; отменяй все стадии при ошибке transform, корректно закрывай каналы.
// 69. Кеш с экспирацией: sync.Map + горутина-сборщик, которая периодически удаляет просроченные ключи.
// 70. Реализация "once per key": concurrent dedup запросов — первый делает работу, остальные ждут результата по каналу.

// --- Блок O. Доп. темы (по желанию) ---

// 71. context.WithValue: прокинь request-id (только для метаданных, не для обязательных параметров).
// 72. time.AfterFunc: отложенный вызов колбэка — обеспечь отмену через таймер.Stop.
// 73. sync.Once: инициализация ресурса, потокобезопасно; сравни с LoadOrStore у sync.Map.
// 74. Паттерн «tee» для каналов: дублируй поток значений в два выходных канала.
// 75. Паттерн «bridge»: объединяй поток из каналов каналов (chan <-chan T) в один.

// Подсказка для самопроверки (без решений):
// • Где нужен close? Только на стороне отправителя.
// • Где нужен Stop()? Всегда у Timer/Ticker.
// • Где возможны гонки? Общие данные без синхронизации; проверяй -race.
// • Когда sync.Map/Pool уместны? Высокая конкуренция чтения/записи или краткоживущие объекты в больших объёмах.
// • Контекст: проверяй ctx.Done() в долгих циклах/блокирующих операциях.
